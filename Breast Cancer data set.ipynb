{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold as kf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history= [ ]\n",
    "        self.w_list = [ ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        eps=10**-10\n",
    "        \n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            \n",
    "            # approximate y with linear combination of weights and x, plus bias\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # apply sigmoid function\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "            \n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # update parameters\n",
    "#             w_prev = self.weights\n",
    "#             w0_prev = self.bias\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "            #appending cost and w values\n",
    "    \n",
    "            cost=-np.sum(y*np.log(y_predicted) + (1-y) * np.log(1 - y_predicted))\n",
    "            self.cost_history.append(cost)\n",
    "            w=list(self.weights)\n",
    "            w.append(self.bias)\n",
    "            self.w_list.append(w)\n",
    "            \n",
    "            #stoping criteria\n",
    "#             if np.sum(np.square(w_prev-self.weights)) < eps:\n",
    "#                 break\n",
    "                \n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        return y_predicted\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def score(self,x_test,y_test):\n",
    "        y_p = self.predict(x_test)\n",
    "        correct = 0\n",
    "        for i in range(len(y_p)):\n",
    "            if y_p[i]==y_test[i]:\n",
    "                correct+=1\n",
    "        return correct/len(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer=pd.read_csv(\"wdbc.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1      2      3       4       5        6        7        8        9   \\\n",
       "0    M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
       "1    M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
       "2    M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
       "3    M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
       "4    M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
       "..  ..    ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "564  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n",
       "565  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n",
       "566  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n",
       "567  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n",
       "568  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n",
       "\n",
       "         10  ...      22     23      24      25       26       27      28  \\\n",
       "0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = cancer.drop([0],axis='columns')\n",
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cancer.iloc[:,1:31]\n",
    "y=np.array([1 if i==\"M\" else 0 for i in cancer[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-09  and Score is : 0.36861471861471873\n",
      "Learning rate: 1e-08  and Score is : 0.36861471861471873\n",
      "Learning rate: 1e-07  and Score is : 0.36861471861471873\n",
      "Learning rate: 1e-06  and Score is : 0.896861471861472\n",
      "Learning rate: 1e-05  and Score is : 0.9114718614718618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity_000\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\adity_000\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001  and Score is : 0.8944805194805197\n"
     ]
    }
   ],
   "source": [
    "k=kf(n_splits=20)\n",
    "lr_range = [10**i for i in range(-9,-3)]\n",
    "for j in lr_range :\n",
    "    a=[ ]\n",
    "    for train_index,test_index in k.split(X_train,y_train):\n",
    "        reg=LogisticRegression(learning_rate=j,n_iters=200)\n",
    "        reg.fit(X_train[train_index],y_train[train_index])\n",
    "        a.append(reg.score(X_train[test_index],y_train[test_index]))\n",
    "        sc=sum(a)/len(a)\n",
    "    print(\"Learning rate:\",j,\" and Score is :\",sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity_000\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\adity_000\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "reg=LogisticRegression(learning_rate=10**-5)\n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(cm):\n",
    "    tn = cm[0][0]\n",
    "    fp = cm[0][1]\n",
    "    fn = cm[1][0]\n",
    "    tp = cm[1][1]\n",
    "    sn = tp/(tp+fn)\n",
    "    sp = tn/(tn+fp)\n",
    "    pr = tp/(tp+fp)\n",
    "    fm = 2*(pr*sn)/(pr+sn)\n",
    "    print(f\"Sensitivity is {sn}\")\n",
    "    print(f\"Specificity is {sp}\")\n",
    "    print(f\"Precission is {pr}\")\n",
    "    print(f\"F_measur is {fm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm , annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity is 0.8909090909090909\n",
      "Specificity is 0.9431818181818182\n",
      "Precission is 0.9074074074074074\n",
      "F_measur is 0.8990825688073394\n"
     ]
    }
   ],
   "source": [
    "calc(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.01873615e-03, -8.49964668e-03, -2.98269082e-02, -1.71025356e-02,\n",
       "       -5.26257044e-05,  6.04832373e-07,  6.32188340e-05,  2.94431931e-05,\n",
       "       -9.82052845e-05, -4.08730374e-05, -3.43799286e-05, -6.54121071e-04,\n",
       "       -8.01201272e-05,  1.06845253e-02, -3.79505834e-06, -1.59012541e-06,\n",
       "       -2.66963731e-07, -8.17867492e-07, -1.04232420e-05, -1.55830758e-06,\n",
       "       -5.10847489e-03, -1.06718251e-02, -2.97705388e-02,  2.05621769e-02,\n",
       "       -6.71260517e-05,  4.37149957e-05,  1.25279624e-04,  3.13279946e-05,\n",
       "       -1.32954380e-04, -4.07570442e-05])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0006750341696355451"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appling normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.58916084e-03, 5.60023310e-03, 3.12062937e-02, ...,\n",
       "        6.22086247e-05, 1.23688811e-04, 3.05944056e-05],\n",
       "       [4.13461538e-03, 6.93764569e-03, 2.70600233e-02, ...,\n",
       "        5.16317016e-05, 1.37645688e-04, 2.98951049e-05],\n",
       "       [2.54254079e-03, 4.61247086e-03, 1.62703963e-02, ...,\n",
       "        3.05944056e-05, 8.52564103e-05, 2.96328671e-05],\n",
       "       ...,\n",
       "       [5.55652681e-03, 7.22902098e-03, 3.73834499e-02, ...,\n",
       "        7.26398601e-05, 1.36072261e-04, 3.02447552e-05],\n",
       "       [4.14335664e-03, 8.11480186e-03, 2.69667832e-02, ...,\n",
       "        2.39481352e-05, 5.50699301e-05, 2.27156177e-05],\n",
       "       [5.70804196e-03, 5.28846154e-03, 3.80827506e-02, ...,\n",
       "        6.54720280e-05, 1.06148019e-04, 2.68735431e-05]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalise(data):\n",
    "    d_min = data.min()\n",
    "    d_max = data.max()\n",
    "    data = (data - d_min) / (d_max - d_min)\n",
    "    return  data,d_min,d_max\n",
    "\n",
    "x_norm , x_min , x_max = normalise(X_train)\n",
    "y_norm , y_min , y_max = normalise(y_train)\n",
    "\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001  and Score is : 0.631385281385281\n",
      "Learning rate: 0.001  and Score is : 0.631385281385281\n",
      "Learning rate: 0.01  and Score is : 0.631385281385281\n",
      "Learning rate: 0.1  and Score is : 0.6548701298701297\n",
      "Learning rate: 1  and Score is : 0.8569264069264071\n",
      "Learning rate: 10  and Score is : 0.896753246753247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity_000\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\adity_000\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 100  and Score is : 0.8471861471861473\n"
     ]
    }
   ],
   "source": [
    "k=kf(n_splits=20)\n",
    "lr_range = [10**i for i in range(-4,3)]\n",
    "for j in lr_range :\n",
    "    a=[ ]\n",
    "    for train_index,test_index in k.split(x_norm,y_norm):\n",
    "        reg=LogisticRegression(learning_rate=j,n_iters=200)\n",
    "        reg.fit(x_norm[train_index],y_norm[train_index])\n",
    "        a.append(reg.score(x_norm[test_index],y_norm[test_index]))\n",
    "        sc=sum(a)/len(a)\n",
    "    print(\"Learning rate:\",j,\" and Score is :\",sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Above data we supposed to take learning parameter as 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_norm,y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.93123543e-03, 6.37237762e-03, 5.30594406e-02, ...,\n",
       "        7.83216783e-05, 8.32167832e-05, 2.35489510e-05],\n",
       "       [5.26806527e-03, 6.36363636e-03, 3.42074592e-02, ...,\n",
       "        2.67511655e-05, 6.90268065e-05, 1.91083916e-05],\n",
       "       [2.86917249e-03, 4.56876457e-03, 1.83566434e-02, ...,\n",
       "        1.90209790e-05, 7.29020979e-05, 2.68327506e-05],\n",
       "       ...,\n",
       "       [3.86655012e-03, 4.30069930e-03, 2.46911422e-02, ...,\n",
       "        2.91666667e-05, 5.90617716e-05, 1.80827506e-05],\n",
       "       [3.21678322e-03, 4.90384615e-03, 2.06643357e-02, ...,\n",
       "        2.16520979e-05, 8.73543124e-05, 2.29632867e-05],\n",
       "       [4.57459207e-03, 5.91783217e-03, 2.94871795e-02, ...,\n",
       "        4.49009324e-05, 1.00145688e-04, 2.51486014e-05]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Normalising the test data using previous max and min value\n",
    "\n",
    "x_norm_test = ( X_test - x_min ) / (x_max - x_min )\n",
    "y_norm_test = (y_test - y_min) / (y_max - y_min)\n",
    "\n",
    "x_norm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9370629370629371"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_norm_test,y_norm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_norm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_norm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
